{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees (for Classification)\n",
    "\n",
    "In a decision tree the data is split at each node according to a decision rule. This corresponds to nested _if-then-else_-rules. In the _if_-part of such a rule are decison is made based on a feature of the data record. \n",
    "\n",
    "We will use the scikit learn implementation. For this implementation the features must be binary or have (at least) ordinal characteristic. If a feature is e.g. nominal with many values, it must be converted to a set of binary (one-hot-coded) features.\n",
    "\n",
    "\n",
    "The splitting rules in the scikit learn implementation are binary and are based on a threshold, e.g.\n",
    "  - _if $x_6 <= 2.14$ then_ left subbranch, _else_ right subbranch.         \n",
    "  - binary features must be coded as 0/1, so the rule becomes: if $x_2 <= 0.5$ _then_ left subbranch, _else_ right subbranch. \n",
    "\n",
    "\n",
    "<!--The structure of the tree will be determined by data (see below) and a training procedure.-->\n",
    "\n",
    "In the leafs of the tree the (class) predictions are made. There are two possibilities for such an inference: \n",
    "   - hard assignment: Predict for the data records which end up on a leaf by the majority class of the training data that end up on that leaf.          \n",
    "   - soft assignment: Assign probabilities according to the distribution of the classes in respect to the training data which end up on that leaf.\n",
    "\n",
    "As an example of a decision tree we will learn the following tree from the titanic data set: \n",
    "<img src=\"./data/titanic.png\" width=\"1000px\"/>\n",
    "\n",
    "A full explanation of the tree will be given later. Here just look at the desion rules (first line of the inner nodes) and at the last line of the leafs. In each leaf you see a array with counts of the different targets (train data): [number_died number_survivors] .\n",
    "\n",
    "### Learning \n",
    "\n",
    "Finding a tree that splits the traing data optimal is np-hard. Therefore often a _greedy_-strategy is used:\n",
    "\n",
    "To build up a decision tree the algorithm starts at the root of the tree. The feature and the threshold \n",
    "that splits the training data best (with respect to the classes) are choosen. In an iterative way the whole tree is build up by such splitting rules. \n",
    "\n",
    "There are different criteria for measuring the \"separation (split) quality\". The most important ones are:\n",
    "\n",
    "- Gini Impurity \n",
    "- Information Gain \n",
    "\n",
    "In this tutorial we use the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Information Gain as splitting criterion\n",
    "\n",
    "The entropy with respect to the target class variable $y$ of a training data set $\\mathcal D$ is defined as:\n",
    "\n",
    "$$\n",
    " H(y, \\mathcal D) = - \\sum_{y \\in \\mathcal Y} p(y|\\mathcal D) \\log_2 p(y|\\mathcal D)\n",
    "$$\n",
    "with the domain of the target values $\\mathcal Y = \\{t_1, t_2,... \\}$.\n",
    "\n",
    "\n",
    "The probabilities are estimated by \n",
    "$$\n",
    "  p(y=t_i, \\mathcal D) = |\\mathcal D^{(y=t_i)}| /|\\mathcal D| \n",
    "$$    \n",
    "\n",
    "\n",
    "with the number of training data $|\\mathcal D|$  and the number of training data $|\\mathcal D^{(y=t_i)}|$ with target label $t_i$: \n",
    "\n",
    "\n",
    "On a node a (binary) split on a feature $x_k$ is made by the split rule $x_k \\leq v$. \n",
    "As result there are two data sets $\\mathcal D_0$ and $\\mathcal D_1$ for the left resp. the right branch.\n",
    "\n",
    "The feature $x_k$ and the split value $v$ are choosen that they maximize the 'reduction of the entropy' measured by the information gain $I$:\n",
    "$$\n",
    "  I(y; x_k) = H(y, \\mathcal D) - H(y|x_k) = H(y, \\mathcal D) - \\sum_{j=0}^1 p_jH(y, \\mathcal D_j) =\n",
    "  H(y, \\mathcal D) + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j|}{|\\mathcal D|} p(y|\\mathcal D_j) \\log_2 p(y|\\mathcal D_j)\n",
    "$$\n",
    "Note that $p_{j=0}$  is the estimated probability that a random data record of $\\mathcal D$ has feature value $x_k \\leq v$ which can be estimated by ${|\\mathcal D_0|}/{|\\mathcal D|}$ (analog for $j=1$).\n",
    "\n",
    "$p(y=t_i|\\mathcal D_0)$ can also be estimated by the fraction of the counts ${|\\mathcal D_0^{(y=t_i)}|}/{|\\mathcal D_0|}$. \n",
    "So the information gain can be computed just with counts:\n",
    "\n",
    "\n",
    "$$\n",
    "  I(y; x_k) = \n",
    "   \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|} + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D_j|}\n",
    "$$\n",
    "\n",
    "\n",
    "<!--$|\\mathcal D_0|$ respectivly $|\\mathcal D_1|$ is the number of elements in the splitted data sets.-->\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Overfitting\n",
    "\n",
    "Deep decision trees generalize often poorly. The following remedies reduce overfitting: \n",
    "\n",
    "- Limitation of the maximal depth of the tree. \n",
    "- Pruning with an validation set either during training (pre-pruning) or after training (post-pruning).\n",
    "- Dimensionality reduction (reducing the number of features before training)\n",
    "\n",
    "\n",
    "Also often combining decision trees to an ensemble (decision forests) is used against overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Survival of the Titanic \n",
    "\n",
    "\n",
    "First you have read in the titanic data with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_df = pd.read_csv(\"titanic-train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`train_df` is a [_pandas_](http://pandas.pydata.org/) [data frame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html). \n",
    "Let's view the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 5 columns):\n",
      "Name        891 non-null object\n",
      "Sex         891 non-null object\n",
      "Ticket      891 non-null object\n",
      "Cabin       204 non-null object\n",
      "Embarked    889 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 34.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Columns with categorical data\n",
    "categorical = train_df.dtypes[train_df.dtypes=='object'].index\n",
    "print(train_df[categorical].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit's learn decision trees can handle only numeric data. So we must convert the nominal `Sex` feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      male\n",
       "1    female\n",
       "2    female\n",
       "3    female\n",
       "4      male\n",
       "Name: Sex, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Sex\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"Sex\"] = np.where(train_df[\"Sex\"].str.contains(\"female\"),1,0) #female=1, male=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: Sex, dtype: int32"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Sex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Survived` is the target, that we want to predict from the values of the other columns.   \n",
    "But not all of the other columns are helpful for classification. So we choose a feature set by hand and convert the features into a numpy array for scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.9250</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0500</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fare  Pclass  Sex   Age  SibSp\n",
       "0   7.2500       3    0  22.0      1\n",
       "1  71.2833       1    1  38.0      1\n",
       "2   7.9250       3    1  26.0      0\n",
       "3  53.1000       1    1  35.0      1\n",
       "4   8.0500       3    0  35.0      0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = targets = labels  =  train_df.Survived\n",
    "\n",
    "columns =  [\"Fare\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\"]\n",
    "\n",
    "train_df[columns].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.25  ,   3.    ,   0.    ,  22.    ,   1.    ],\n",
       "       [ 71.2833,   1.    ,   1.    ,  38.    ,   1.    ],\n",
       "       [  7.925 ,   3.    ,   1.    ,  26.    ,   0.    ],\n",
       "       ..., \n",
       "       [ 23.45  ,   3.    ,   1.    ,      nan,   1.    ],\n",
       "       [ 30.    ,   1.    ,   0.    ,  26.    ,   0.    ],\n",
       "       [  7.75  ,   3.    ,   0.    ,  32.    ,   0.    ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = train_df.as_matrix(columns=columns) # or .val\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Age', 'Cabin', 'Embarked'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[:, train_df.isnull().any()].columns.values #Für uns nur Age interessant, 4. Spalte in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.69911764705882"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are missing values (`nan`). We use the scikit learn `Imputer` to replace them by the mean of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.25      ,   3.        ,   0.        ,  22.        ,   1.        ],\n",
       "       [ 71.2833    ,   1.        ,   1.        ,  38.        ,   1.        ],\n",
       "       [  7.925     ,   3.        ,   1.        ,  26.        ,   0.        ],\n",
       "       ..., \n",
       "       [ 23.45      ,   3.        ,   1.        ,  29.69911765,   1.        ],\n",
       "       [ 30.        ,   1.        ,   0.        ,  26.        ,   0.        ],\n",
       "       [  7.75      ,   3.        ,   0.        ,  32.        ,   0.        ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "mean_imputer = mean_imputer.fit(features)\n",
    "features = mean_imputer.transform(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we are ready to learn a decision tree by the criterion 'Information Gain' and we restrict the depth of the tree to 3.\n",
    "We use the [scikit learn decison tree module](http://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "We use a splitting criterion the 'Information Gain'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822671156004\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "target = y\n",
    "tree_clf = clf.fit(features, target)\n",
    "print(tree_clf.score(features, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12330431,  0.18665493,  0.5670424 ,  0.09423074,  0.02876762])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"Fare\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\"\n",
    "tree_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`clf` is an instance of a trained decision tree classifier.\n",
    "\n",
    "The decision tree can be visualized. For this we must write an graphviz dot-File  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "\n",
    "with open(\"data/titanic.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f, feature_names=columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/titanic_.png'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import pydotplus\n",
    "dot_data = tree.export_graphviz(clf,\n",
    "                                feature_names=columns,\n",
    "                                out_file=None,\n",
    "                                rounded=True,\n",
    "                               filled=True)\n",
    "\n",
    "    \n",
    "import graphviz\n",
    "src = graphviz.Source(dot_data, format=\"png\")\n",
    "src.render(\"data/titanic_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dot file can be converted with the graphiz `dot`- renderer to an image.\n",
    "\n",
    "    dot -Tpng titanic.dot -o titanic.png\n",
    "\n",
    "Here is the graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./data/titanic.png\" width=\"1000px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Classifier berücksichtigt den Target (\"Survived\", \"Not Survived\"): \n",
    "- 891 Passagiere insgesamt werden anhand der übergebenen Features analysiert\n",
    "    - 549 sind gestorben\n",
    "    - 342 überlebten\n",
    "Der Classifiert ermittelt an dieser Stelle die \"beste\" Feature (Man Criterion), um den Weg nach unten zu laufen: Sex. Die Trennlinie bei x[Sex] liegt bei 0.5 (da die Werte entweder 0 oder 1 sind). \n",
    "\n",
    "Es werden 577 Männer im ganzen Schiff und 314 Frauen ermittelt.\n",
    "\n",
    "Linker Teilbaum (Men):\n",
    "\n",
    "_Neues Hauptkriterion ist Fare_:\n",
    "- Unter Berücksichtigung der Männer (577) als Samples ergibt sich das Fare (Ticketkosten) als main criterion und eine Trennlinie bei 26.269\n",
    "    - 468 Männer haben weniger als 26.269 ausgegeben (arme Männer)\n",
    "    - 109 Männer haben mehr als 26.269 ausgegeben (reiche Männer)\n",
    "    - --> Grafica uomini poveri e ricchi\n",
    "_Neues Hauptkriterion bzgl. Fare ist Alter_ und Trennlinie bei 13,5:\n",
    "Von den 577 Männer, wie viele waren arm und jünger als 13,5? 415 Männer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./data/titanic_.png\" width=\"1200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "According to the decision tree the main criterion (root node) for survival is the sex of the passenger. In the left subtree are the male passengers (sex = 0), in the right subtree the female (sex=1). \n",
    "\n",
    "In the leafs the class information is given by a `value` array. Here the second value is the number of survivers in the leafs.\n",
    "\n",
    "For example the leftmost leaf represents passengers that are male (sex=0) with fare<=26.2687 and age<=13.5. 13 of such boys survived and 2 of them died.\n",
    "\n",
    "The entropy $- \\sum p_i \\log_2 (p_i)$ is displayed also at each node (splitting criterion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises: Splitting criterion entropy / information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total passengers:  891\n"
     ]
    }
   ],
   "source": [
    "samples = passengers = 891\n",
    "print(\"Total passengers: \", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dead people:  549\n",
      "Total survived people:  342\n"
     ]
    }
   ],
   "source": [
    "#How many survivors? How many dead people?\n",
    "#Values in the root node \n",
    "dead, survived = train_df.Survived.value_counts()\n",
    "print(\"Total dead people: \", dead)\n",
    "print(\"Total survived people: \", survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of men:  577\n",
      "Total number of women:  314\n"
     ]
    }
   ],
   "source": [
    "#How many men? How many women?\n",
    "#Value of samples in the first child nodes applied on the total samples of the root\n",
    "tot_men, tot_women = train_df.Sex.value_counts() #0: 577, 1: 349\n",
    "print(\"Total number of men: \", tot_men)\n",
    "print(\"Total number of women: \", tot_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link part of the tree (Men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men survived:  109\n",
      "Men dead:  468\n"
     ]
    }
   ],
   "source": [
    "#How many men survived? \n",
    "stats_men = train_df.Survived[train_df.Sex == 0].value_counts()\n",
    "men = train_df.Survived[train_df.Sex == 0]\n",
    "#Value in second node in the tree - left part\n",
    "print(\"Men survived: \", stats_men[1])\n",
    "print(\"Men dead: \", stats_men[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rich men on board:  162\n",
      "0    107\n",
      "1     55\n",
      "Name: Survived, dtype: int64\n",
      "\n",
      "Poor men on board:  415\n",
      "0    361\n",
      "1     54\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#How many rich men? How many poor men?\n",
    "rich_men = train_df.Survived[(train_df.Sex == 0) & (train_df.Fare > 26.269)]\n",
    "print(\"Rich men on board: \",len(rich_men))\n",
    "print(rich_men.value_counts())\n",
    "print(\"\")\n",
    "#samples in the third child node on the left side\n",
    "poor_men = train_df.Survived[(train_df.Sex == 0) & (train_df.Fare <= 26.269)]\n",
    "print(\"Poor men on board: \", len(poor_men))\n",
    "print(poor_men.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total - Poor young men:  15\n",
      "1    13\n",
      "0     2\n",
      "Name: Survived, dtype: int64\n",
      "Poor young died men:  2\n",
      "Poor young survived men:  13\n"
     ]
    }
   ],
   "source": [
    "#How many young and poor men died?\n",
    "poor_young_men = train_df.Survived[(train_df.Sex == 0) & (train_df.Fare <= 26.269) & (train_df.Age <= 13.5)]\n",
    "print(\"Total - Poor young men: \", len(poor_young_men))\n",
    "\n",
    "stats_poor_young_men = poor_young_men.value_counts()\n",
    "print(stats_poor_young_men)\n",
    "print(\"Poor young died men: \", stats_poor_young_men[0])\n",
    "print(\"Poor young survived men: \", stats_poor_young_men[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right part of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women survived:  233\n",
      "Women dead:  81\n"
     ]
    }
   ],
   "source": [
    "#How many women survived? \n",
    "stats_women = train_df.Survived[train_df.Sex == 1].value_counts()\n",
    "#Value in second node in the tree - right side\n",
    "print(\"Women survived: \", stats_women[1])\n",
    "print(\"Women dead: \", stats_women[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1: \n",
    "\n",
    "Compute the root node entropy (with numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_probs(X):\n",
    "    return [np.mean(X == c) for c in set(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(rate):\n",
    "    return -1*np.sum(rate * np.log2(rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conditional_entropy(p, cond_p):\n",
    "    result = 0\n",
    "    result -= np.sum(p*cond_p*np.log2(cond_p))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total passengers:  891\n",
      "Total men on board:  577\n",
      "Total women on board:  314\n"
     ]
    }
   ],
   "source": [
    "print(\"Total passengers: \", len(train_df))\n",
    "men, women = train_df.Sex.value_counts()\n",
    "print(\"Total men on board: \", men)\n",
    "print(\"Total women on board: \", women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([549, 342], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survival_rate = train_df.Survived.value_counts().values\n",
    "survival_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61616162,  0.38383838])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survival_rate_P = train_df.Survived.value_counts(normalize=True).values\n",
    "survival_rate_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.960707901876\n"
     ]
    }
   ],
   "source": [
    "#Entropy at root\n",
    "entropy_root = entropy(survival_rate_P)\n",
    "print(entropy_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Compute the information gain of the first split node (root node). Use the entropy values and the number of data records (samples)\n",
    "from the decision tree image\n",
    "$\\text{Information gain} = \\text{(Entropy of distribution before the split)} – \\text{(entropy of distribution after it)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex  Survived\n",
       "0    0           468\n",
       "     1           109\n",
       "1    1           233\n",
       "     0            81\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Values Survived by Sex\n",
    "#equivalent to train_df.groupby('Sex').Survived.value_counts()\n",
    "survived_by_sex = train_df.groupby('Sex')['Survived'].value_counts()\n",
    "survived_by_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex  Survived\n",
      "0    0           0.811092\n",
      "     1           0.188908\n",
      "1    1           0.742038\n",
      "     0           0.257962\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Percentage - Sex and survival rate - wrt the total amount of women or men\n",
    "survived_by_sex_P = train_df.groupby('Sex')['Survived'].value_counts(normalize=True)\n",
    "#survived_by_sex_P = train_df.Survived[train_df['Sex'] == 1].value_counts(normalize=True)\n",
    "print(survived_by_sex_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(men):  0.6991817891208407\n",
      "H(women):  0.8236550739295191\n"
     ]
    }
   ],
   "source": [
    "#Entropies (on the child node of the tree)\n",
    "men_entropy = entropy(survived_by_sex_P[0])\n",
    "women_entropy = entropy(survived_by_sex_P[1])\n",
    "\n",
    "print(\"H(men): \", men_entropy)\n",
    "print(\"H(women): \", women_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage men and women on board: \n",
      "Men:  0.64758698092\n",
      "Women:  0.35241301908\n"
     ]
    }
   ],
   "source": [
    "#Percentage men and women on board\n",
    "men_freq, women_freq = train_df.Sex.value_counts(normalize=True)\n",
    "\n",
    "print(\"Percentage men and women on board: \")\n",
    "print(\"Men: \" , men_freq)\n",
    "print(\"Women: \", women_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate men/women: \n",
      "Men:\n",
      " Sex  Survived\n",
      "0    0           0.811092\n",
      "     1           0.188908\n",
      "Name: Survived, dtype: float64\n",
      "Women:\n",
      " Sex  Survived\n",
      "1    1           0.742038\n",
      "     0           0.257962\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#percentage survived men/women\n",
    "print(\"Survival rate men/women: \")\n",
    "men_surv_rate = survived_by_sex_P[:2]\n",
    "women_surv_rate = survived_by_sex_P[2:]\n",
    "print(\"Men:\\n\", men_surv_rate)\n",
    "print(\"Women:\\n\", women_surv_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45278102393122904"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men_cond_entropy = conditional_entropy(men_freq,men_surv_rate)\n",
    "men_cond_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29026677128380357"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "women_cond_entropy = conditional_entropy(women_freq,women_surv_rate)\n",
    "women_cond_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7430477952150326"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_conditional_entropy = men_cond_entropy + women_cond_entropy\n",
    "total_conditional_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21766010666061431"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Information gain\n",
    "\n",
    "entropy_root - total_conditional_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "Compute the information gain of the following split table:\n",
    "\n",
    "|  | class 0  | class 1  |  \n",
    "|---|---|---|\n",
    "| feature <= v| 2  | 13  |      \n",
    "| feature > v | 359  |  41 |   \n",
    "\n",
    "The numbers are the corresponding data records, e.g. there are 13 data records with target class 1 and feature <= v. \n",
    "\n",
    "Write a python function that computes the information gain.The data is given by a python array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array([[2.,13.],[359., 41.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y=0</th>\n",
       "      <th>y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x=0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x=1</th>\n",
       "      <td>359.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y=0   y=1\n",
       "x=0    2.0  13.0\n",
       "x=1  359.0  41.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data, columns=[\"y=0\", \"y=1\"], index=[\"x=0\", \"x=1\"])\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def information_gainY(data):\n",
    "    N = np.sum(data)\n",
    "    print(\"Data: \", N)\n",
    "    #P(Y)\n",
    "    Px = np.sum(data, axis=1)/N\n",
    "    print(\"P(X): \", Px)\n",
    "    #P(X)\n",
    "    Py =np.sum(data, axis=0)/N\n",
    "    print(\"P(Y): \", Py)\n",
    "    #conditional probabilities\n",
    "    cond_probs = data.T/np.sum(data,axis=1) #tr\n",
    "    print(\"Cond probs:\", cond_probs)\n",
    "   \n",
    "    #P(Y|X)\n",
    "    P_y_x = cond_probs*np.log2(cond_probs)\n",
    "    #print(\"P(Y|X): \", P_y_x)\n",
    "    \n",
    "    #H(Y|X)\n",
    "    H_y_x = -np.sum(Px*P_y_x)\n",
    "    \n",
    "    #H(Y)\n",
    "    H_y = -np.sum(Py*np.log2(Py))\n",
    "    print(\"Entropy: \",H_y)\n",
    "    print(\"Conditional entropy: \",H_y_x)\n",
    "    #IG\n",
    "    IG = H_y - H_y_x\n",
    "    print(\"Information Gain: \", IG)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2.  359.]\n",
      " [  13.   41.]]\n",
      "[  15.  400.]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[2.,13.],[359., 41.]])\n",
    "print(data.T)\n",
    "print(np.sum(data,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  415.0\n",
      "P(X):  [ 0.03614458  0.96385542]\n",
      "P(Y):  [ 0.86987952  0.13012048]\n",
      "Cond probs: [[ 0.13333333  0.8975    ]\n",
      " [ 0.86666667  0.1025    ]]\n",
      "Entropy:  0.557768514612\n",
      "Conditional entropy:  0.48011063657\n",
      "Information Gain:  0.0776578780428\n"
     ]
    }
   ],
   "source": [
    "information_gainY(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction\n",
    "\n",
    "To make predictions with scikit learn, we must convert the data in\n",
    "the same way as we did it with the training data. Then we could use the method:    \n",
    "\n",
    "    clf.predict(testdata)\n",
    "    \n",
    "Note: The depth of the decision tree is a hyperparameter. So the depth should be determined with the help of a \n",
    "validation set or by cross validation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def information_gainY(data):\n",
    "    N = np.sum(data)\n",
    "    print(\"Data: \", N)\n",
    "    #P(Y)\n",
    "    Py = np.sum(data, axis=1)/N\n",
    "    print(\"P(Y): \", Py)\n",
    "    #P(X)\n",
    "    Px =np.sum(data, axis=0)/N\n",
    "    print(\"P(X): \", Px)\n",
    "    #conditional probabilities\n",
    "    cond_probs = data/np.sum(data,axis=0) #tr\n",
    "    print(\"Cond probs:\", cond_probs)\n",
    "   \n",
    "    #P(Y|X)\n",
    "    P_y_x = cond_probs*np.log2(cond_probs)\n",
    "    #print(\"P(Y|X): \", P_y_x)\n",
    "    \n",
    "    #H(Y|X)\n",
    "    H_y_x = -np.sum(Px*P_y_x)\n",
    "    \n",
    "    #H(Y)\n",
    "    H_y = -np.sum(Py*np.log2(Py))\n",
    "    print(\"Entropy: \",H_y)\n",
    "    print(\"Conditional entropy: \",H_y_x)\n",
    "    #IG\n",
    "    IG = H_y - H_y_x\n",
    "    print(\"Information Gain: \", IG)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Literature\n",
    "\n",
    "- [Cri] [A. Criminisi, J. Shotton, and E. Konukoglu, Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi- Supervised Learning, no. MSR-TR-2011-114, 28 October 2011.](http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
